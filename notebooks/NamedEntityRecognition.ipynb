{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOyoN6YCpWnJ"
      },
      "source": [
        "# Named entity recognition - fine-tuning BERT\n",
        "\n",
        "In this notebook we'll take a look at the process needed to fine-tine a pretrained [BERT](https://arxiv.org/abs/1810.04805) model to recognize named entities in text. \n",
        "\n",
        "Named entity recognition is a token classification task which means that we classify each token into one of the corresponding classes. Usually these classes are entity types such as *person*, *organization*, *location*, etc., and we have a special category for token that don't belong to any entity type: *other*.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/andrejmiscic/NLP-workshop/raw/master/figures/ner.PNG\" width=\"700\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm7Aa8bKzomB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKH-fJ4BqHmB"
      },
      "source": [
        "First things first, let's make sure we have a GPU instance in this Colab session:\n",
        "- `Edit -> Notebook settings -> Hardware accelerator` must be set to GPU\n",
        "- if needed, reinitiliaze the session by clicking `Connect` in top right corner\n",
        "\n",
        "After the session is initilized, we can check our assigned GPU with the following command (fingers crossed it's a Tesla P100!!):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk9e8LdLox4J"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xx4wOgvqSXA"
      },
      "source": [
        "Let's install some additional libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LxWXxu0LAs0"
      },
      "source": [
        "!wget https://github.com/andrejmiscic/NLP-workshop/raw/master/utils/ner_utils.py\r\n",
        "!wget https://github.com/andrejmiscic/NLP-workshop/raw/master/utils/trainer.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy_H_EX5ozlv"
      },
      "source": [
        "!pip install transformers  # pretrained BERT model\n",
        "!pip install gdown  # loading from Drive\n",
        "!pip install seqeval  # NER evaluation\n",
        "!pip install termcolor  # NER visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_wUnWMio1ID"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from ner_utils import TokenClassificationDataset, collate_dict_batch_to_tensors, align_predictions_and_labels, token_cls_evaluate\n",
        "from termcolor import colored\n",
        "from trainer import Trainer, RunConfig\n",
        "from transformers import DistilBertConfig, DistilBertModel, DistilBertTokenizerFast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT0AjKyiqzjk"
      },
      "source": [
        "## Data\n",
        "\n",
        "We are working with the commonly used [CoNLL-2003](https://www.aclweb.org/anthology/W03-0419.pdf) NER task which has been established as a benchmark to evaluate new approaches. It consists of Reuters news articles and contains four different entity types: person (PER), location (LOC), organization (ORG) and other miscellaneous entities (MISC).\n",
        "\n",
        "Let's load the data and look at an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX8R3_eio2xe"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/CoNLLP-NER/conllpp_train.txt\n",
        "!wget https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/CoNLLP-NER/conllpp_dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVIuQPduo7UZ"
      },
      "source": [
        "with open(\"/content/conllpp_train.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "for i in range(2,15):  # prints first two examples\n",
        "  print(lines[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl9Da6odTkh0"
      },
      "source": [
        "# max length of input, pretrained model only supports max_len up to 512, use smaller values for faster training\r\n",
        "max_len = 512\r\n",
        "\r\n",
        "# we use tokenizer to prepare the inputs\r\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\r\n",
        "\r\n",
        "train_dataset = TokenClassificationDataset(\"/content/conllpp_train.txt\", tokenizer, max_len)\r\n",
        "val_dataset = TokenClassificationDataset(\"/content/conllpp_dev.txt\", tokenizer, max_len)\r\n",
        "\r\n",
        "# we'll use this mapping to convert from model class id to human readable class\r\n",
        "class_list = train_dataset.class_list\r\n",
        "print(f\"Classes: {class_list}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy-mCJucPeQ3"
      },
      "source": [
        "Looking at the classes, we can see that we have all the aforementioned entity types. Also notice prefixes *B* and *I*, which denote whether a particular word is at the *beginning* of the entity or *inside* it.\r\n",
        "\r\n",
        "Tokenizer helps us to convert our input sentences into a format that BERT will understand. Let's look at an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUkFLkrtRYqX"
      },
      "source": [
        "example = \"This is an example of how to use a tokenizer.\"\r\n",
        "\r\n",
        "# converts inputs to tokens from the vocabulary\r\n",
        "tokens = tokenizer.tokenize(example)\r\n",
        "print(f\"Tokens: {tokens}\")\r\n",
        "\r\n",
        "# converts tokens to indices in the vocabulary\r\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "print(f\"Ids: {ids}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2grT0XQ20IBq"
      },
      "source": [
        "## Model\n",
        "\n",
        "We've now implemented everything needed for the data side of the pipeline, let's now look at our model. The simplicity of using BERT for most of downstream tasks lies in the fact that we can just add a classification layer on top of produced representations and achieve good performance. To fine-tune the obtained model we update the combined parameters of both BERT and classification layer on the downstream dataset.\n",
        "\n",
        "For the purposes of this workshop, we won't directly work with BERT model as we are constrained by computational power and time. We rather opt out for [DistilBERT](https://arxiv.org/abs/1910.01108). DistilBERT is a smaller version of BERT (same architecture, less layers) that is trained by distilling the knowledge of a large BERT model to the smaller model. It is much faster and retains most of the representational power of BERT base model, so it's perfect for our use case.\n",
        "\n",
        "Named entity recogntion is a token classification task and a special version of DistillBERT for this type of downstream tasks is already implemented in *transformers* library, called [*DistilBertForTokenClassification*](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertfortokenclassificationps://). For demonstrational purposes we reimplement a DistillBERT with a classification head in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZv72PYjo_9t"
      },
      "source": [
        "class DistilBertTokenClassificationModel(nn.Module):\n",
        "  def __init__(self, bert_config, num_classes, dropout_prob=0.1):\n",
        "    super(DistilBertTokenClassificationModel, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.bert = DistilBertModel(bert_config)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.classification_layer = nn.Linear(in_features=bert_config.hidden_size, out_features=num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "    x = self.bert(input_ids, attention_mask)[0]  # produces token representations\n",
        "    x = self.dropout(x)  # mitigates overfitting\n",
        "    logits = self.classification_layer(x)  # classifies tokens into entity types\n",
        "\n",
        "    if labels is None:\n",
        "      return logits\n",
        "\n",
        "    # compute the loss\n",
        "    loss = nn.CrossEntropyLoss()(logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\n",
        "    return (loss, logits)\n",
        "\n",
        "  def load(self, path_to_dir):\n",
        "    self.bert = DistilBertModel.from_pretrained(path_to_dir)\n",
        "    model_path = os.path.join(path_to_dir, \"model.tar\")\n",
        "    if os.path.exists(model_path):\n",
        "      checkpoint = torch.load(model_path)\n",
        "      self.dropout.load_state_dict(checkpoint[\"dropout\"])\n",
        "      self.classification_layer.load_state_dict(checkpoint[\"cls\"])\n",
        "    else:\n",
        "      print(\"No model.tar in provided directory, only loading bert model.\")\n",
        "\n",
        "  def save_pretrained(self, path_to_dir):\n",
        "    self.bert.save_pretrained(path_to_dir)\n",
        "    torch.save(\n",
        "        {\"dropout\": self.dropout.state_dict(), \"cls\": self.classification_layer.state_dict()},\n",
        "        os.path.join(path_to_dir, \"model.tar\")\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncALuE6Q38q6"
      },
      "source": [
        "## Training\n",
        "\n",
        "We have now implemented everything to start fine-tuning. We can save the fine-tuned models to our Colab instance (available under `/content/`) or we can connect our Google Drive to Colab and use it as external memory. If you want to do the latter, run the cell below and follow instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrRbAfLYpE5C"
      },
      "source": [
        "# optional if you want to save your models to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN5SYLKD4aRI"
      },
      "source": [
        "Let's set the training parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsYmHzWCpGK3"
      },
      "source": [
        "run_config = RunConfig(\n",
        "    learning_rate = 3e-5,\n",
        "    batch_size = 32,  # start with 32 and decrease if you get CUDA out of memory exception\n",
        "    num_epochs = 3,\n",
        "    output_dir = \"/content/drive/MyDrive/NLP-workshop/BERT-NER/\",\n",
        "    collate_fn = collate_dict_batch_to_tensors\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9AgLbNK40Qf"
      },
      "source": [
        "Instatiate the model and start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcvDv1JxpKOQ"
      },
      "source": [
        "model = DistilBertTokenClassificationModel(\n",
        "    DistilBertConfig.from_pretrained(\"distilbert-base-uncased\"), \n",
        "    num_classes=len(class_list)\n",
        ")\n",
        "model.load(\"distilbert-base-uncased\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jdzy7a0pLiQ"
      },
      "source": [
        "trainer = Trainer(model)\n",
        "trainer.train(train_dataset, val_dataset, device, run_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjaw2rAW5VHA"
      },
      "source": [
        "If you happen to get a CUDA out of memory exception, do the following:\n",
        "- cause another exception so python doesn't hold any references to trainer or model, e.g. run the bottom cell causing ZeroDivisionError\n",
        "- run the cell below that empties GPU cache\n",
        "- decrease the batch_size in run_config and rerun that cell\n",
        "- reinstantiate the model and rerun training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXBNa99RpMx1"
      },
      "source": [
        "1 / 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glVXiIQEpN8K"
      },
      "source": [
        "model = None\n",
        "trainer = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOduwhj20PnO"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "After fine-tuning we have a BERT model specialized for detecting named entities in text. Let's see how it performs on the test set. For the purposes of this workshop we've prepared a model that is already fine-tuned on CoNLL. You can get all the necessary files for evaluation by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPWE_PAXpSDN"
      },
      "source": [
        "!mkdir /content/bert-ner\n",
        "!wget https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/CoNLLP-NER/conllpp_test.txt\n",
        "!gdown -O /content/bert-ner/config.json https://drive.google.com/uc?id=1Tg_sFaL9Ouye8d6l6gJKFYgOJL3BpI7J\n",
        "!gdown -O /content/bert-ner/model.tar https://drive.google.com/uc?id=1-5PKbK88VjIyHPJD1QZea09MCEzkERzT\n",
        "!gdown -O /content/bert-ner/pytorch_model.bin https://drive.google.com/uc?id=1-78MPCczYFLDaZD7gIz4qFglN0b1INMJ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD7Ein_j1PMJ"
      },
      "source": [
        "Let's instantiate everything we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfAxunT9AV1O"
      },
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\r\n",
        "max_len = 512\r\n",
        "test_dataset = TokenClassificationDataset(\"/content/conllpp_test.txt\", tokenizer, max_len)\r\n",
        "class_list = train_dataset.class_list\r\n",
        "id2label = {i: label for i, label in enumerate(class_list)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhzQpWsWpU-1"
      },
      "source": [
        "# only run if you want to use the model we've already fine-tuned for you\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DistilBertTokenClassificationModel(\n",
        "    DistilBertConfig.from_pretrained(\"distilbert-base-uncased\"), \n",
        "    num_classes=len(class_list)\n",
        ")\n",
        "model.load(\"/content/bert-ner/\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDmqKOjtfs-2"
      },
      "source": [
        "Evaluation of our model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AZ8iluhpYkq"
      },
      "source": [
        "logloss, f1 = token_cls_evaluate(model, test_dataset, device, id2label)\n",
        "print(f\"\\nTest log loss = {logloss:.4f}\\nTest F1-score = {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV5xlxVu11Jn"
      },
      "source": [
        "With a F1-score of 0.90 we achieve quite okay performance. Let's now evaluate our model on some extra data, we've selected some BBC articles for this, but feel free to experiment!\n",
        "\n",
        "Sources for articles:\n",
        "\n",
        "- https://www.bbc.com/sport/formula1/54316085\n",
        "- https://www.bbc.com/news/entertainment-arts-54292947\n",
        "- http://www.bbc.com/travel/story/20200914-in-guatemala-the-maya-world-untouched-for-centuries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDvhlpXxpcRK"
      },
      "source": [
        "label2color = {\n",
        "    \"B-PER\" : \"red\",\n",
        "    \"I-PER\" : \"red\",\n",
        "    \"B-ORG\" : \"blue\",\n",
        "    \"I-ORG\" : \"blue\",\n",
        "    \"B-LOC\" : \"green\",\n",
        "    \"I-LOC\" : \"green\",\n",
        "    \"B-MISC\" : \"yellow\",\n",
        "    \"I-MISC\" : \"yellow\",\n",
        "    \"O\" : \"white\"\n",
        "}\n",
        "\n",
        "def tag_some_text(text, show_legend=True):\n",
        "    words = text.split()\n",
        "    inputs = TokenClassificationDataset.convert_example_to_inputs(tokenizer, words, class_list=class_list)\n",
        "    input_ids = torch.tensor([inputs[\"input_ids\"]], dtype=torch.long).to(device)\n",
        "    attention_mask = torch.tensor([inputs[\"attention_mask\"]], dtype=torch.long).to(device)\n",
        "      \n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids, attention_mask)\n",
        "    predictions = np.argmax(logits.cpu().numpy(), axis=2)\n",
        "    predictions, _ = align_predictions_and_labels(predictions, np.array([inputs[\"labels\"]]), id2label)\n",
        "    colors = list(map(label2color.get, predictions[0]))\n",
        "    colored_words = []\n",
        "    for i in range(len(words)):\n",
        "      colored_words.append(colored(words[i], colors[i])) \n",
        "    print(\" \".join(colored_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV41Qc6og7Cr"
      },
      "source": [
        "# word wrap\r\n",
        "from IPython.display import HTML, display\r\n",
        "\r\n",
        "def set_css():\r\n",
        "  display(HTML('''\r\n",
        "  <style>\r\n",
        "    pre {\r\n",
        "        white-space: pre-wrap;\r\n",
        "    }\r\n",
        "  </style>\r\n",
        "  '''))\r\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pTMD1-fpnDs"
      },
      "source": [
        "tag_some_text(\"Lewis Hamilton's quest for the all-time record of Formula 1 wins was put on hold when he was hit with penalties at the Russian Grand Prix. Hamilton's Mercedes team-mate Valtteri Bottas dominated after the world champion was given a 10-second penalty for doing two illegal practice starts. Bottas was on the better strategy - starting on the medium tyres while Hamilton was on softs after a chaotic qualifying session for the Briton - and was tracking Hamilton in the early laps waiting for the race to play out. Behind the top three, Racing Point's Sergio Perez and Renault's Daniel Ricciardo had equally lonely races, the Australian having sufficient pace to overcome a five-second penalty for failing to comply with rules regarding how to rejoin the track when a car runs wide at Turn Two. Ferrari's Charles Leclerc made excellent use of a long first stint on the medium tyres to vault up from 11th on the grid to finish sixth, ahead of the second Renault of Esteban Ocon, the Alpha Tauris of Daniil Kvyat and Pierre Gasly and Alexander Albon's Red Bull. What's next? The Eifel Grand Prix on 11 October as the Nurburgring returns to the F1 calendar for the first time since 2013. The 24-hour touring car race there this weekend has been hit with miserable wet and wintery conditions in the Eifel mountains. Will F1 face the same?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MroHaWlfpovt"
      },
      "source": [
        "tag_some_text(\"Sir David Attenborough has broken Jennifer Aniston's record for the fastest time to reach a million followers on Instagram. At 94 years young, the naturalist's follower count raced to seven figures in four hours 44 minutes on Thursday, according to Guinness World Records. His debut post said: \\'Saving our planet is now a communications challenge.\\' Last October, Friends star Aniston reached the milestone in five hours and 16 minutes. Sir David's Instagram debut precedes the release of a book and a Netflix documentary, both titled A Life On Our Planet.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esoMavJ5pqJ7"
      },
      "source": [
        "tag_some_text(\"Using Lidar, in 2016 the Foundation for Maya Cultural and Natural Heritage launched the largest archaeological survey ever undertaken of the Maya lowlands. In the first phase, whose results were published in 2018, they mapped 2,100km of the Maya Biosphere Reserve. Their hope in the further phases – the second one of which took place in summer 2019, while I was there – is to triple the coverage area. That would make the project the largest Lidar survey not only in Central America, but in the world.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}