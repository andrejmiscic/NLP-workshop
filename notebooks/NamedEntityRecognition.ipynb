{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ner.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPsc0sm9n8QV2jbJ1TdUWT8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FOyoN6YCpWnJ"},"source":["# Named entity recognition - fine-tuning BERT\n","\n","In this notebook we'll take a look at the process needed to fine-tine a pretrained [BERT](https://arxiv.org/abs/1810.04805) model to recognize named entities in text. "]},{"cell_type":"markdown","metadata":{"id":"TKH-fJ4BqHmB"},"source":["First things first, let's make sure we have a GPU instance in this Colab session:\n","- `Edit -> Notebook settings -> Hardware accelerator` must be set to GPU\n","- if needed, reinitiliaze the session by clicking `Connect` in top right corner\n","\n","After the session is initilized, we can check our assigned GPU with the following command (fingers crossed it's a Tesla P100!!):"]},{"cell_type":"code","metadata":{"id":"Vk9e8LdLox4J"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xx4wOgvqSXA"},"source":["Let's install some additional libraries: *transformers* for BERT implementation, *gdown* for loading from Drive, *seqeval* for token level evaluation and *termcolor* for colorful output."]},{"cell_type":"code","metadata":{"id":"qy_H_EX5ozlv"},"source":["!pip install transformers\n","!pip install gdown\n","!pip install seqeval\n","!pip install termcolor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_wUnWMio1ID"},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import DistilBertConfig, DistilBertModel, DistilBertTokenizerFast, get_linear_schedule_with_warmup, AdamW\n","from sklearn.model_selection import train_test_split\n","import os\n","from tqdm import tqdm, trange\n","import numpy as np\n","import pandas as pd\n","from dataclasses import dataclass\n","import gc\n","import pickle\n","from seqeval.metrics import f1_score\n","from termcolor import colored"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LT0AjKyiqzjk"},"source":["## Data\n","\n","We are working with the famous [CoNLL-2003](https://www.aclweb.org/anthology/W03-0419.pdf) NER task which has established as a benchmark to evaluate new approaches to NER. It consists of Reuters news collections and contains four different entity types: person (PER), location (LOC), organization (ORG) and other miscellaneous entities (MISC).\n","\n","Let's load the data and look at an example."]},{"cell_type":"code","metadata":{"id":"oX8R3_eio2xe"},"source":["!gdown https://drive.google.com/uc?id=1q_EUUDtZpR5OWG-LukjTYdIc1tTSXjtY\n","!gdown https://drive.google.com/uc?id=1PChc0nIp2oLuLuhOB_dKLQ-dnJ1_dbjX"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVIuQPduo7UZ"},"source":["with open(\"/content/conllpp_train.txt\", \"r\") as f:\n","  lines = f.readlines()\n","\n","for i in range(2,15):  # prints first two examples\n","  print(lines[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuAjkNZXo9zy"},"source":["class TokenClassificationDataset(Dataset):\n","  def __init__(self, data_path, max_seq_len, label2id=None, id2label=None, sep_token=\"[SEP]\", cls_token=\"[PAD]\", pad_token=\"[PAD]\"):\n","    # special tokens in BERT: sep separates two sentences, cls contains representation for \n","    # sequence class., pad is used to extend inputs to the same length\n","    self.sep_token, self.cls_token, self.pad_token = sep_token, cls_token, pad_token\n","    self.ignore_label_id = nn.CrossEntropyLoss().ignore_index  # we use this to ignore padding tokens when computing loss\n","    self.max_seq_len = max_seq_len\n","    self.label2id, self.id2label = label2id, id2label\n","    \n","    # reads sentences/labels from dataset file\n","    examples, self.unique_labels = self.read_examples_labels_from_file(data_path)\n","\n","    if label2id is None or id2label is None:\n","      self.label2id = {label: i for i, label in enumerate(self.unique_labels)}\n","      self.id2label = {i: label for i, label in enumerate(self.unique_labels)}\n","      \n","    self.inputs = self.convert_examples_to_inputs(examples)\n","\n","  def __len__(self):\n","    return len(self.inputs)\n","\n","  def __getitem__(self, i):\n","    return self.inputs[i]\n","\n","  def read_examples_labels_from_file(self, data_path):\n","    examples = []\n","    unique_labels = set()\n","    with open(data_path, encoding=\"utf-8\") as f:\n","      words, labels = [], []\n","      for line in f:\n","        if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","          if words:\n","            examples.append({\"words\": words, \"labels\": labels})\n","            words, labels = [], []\n","        else:\n","          splits = line.strip().split(\" \")\n","          words.append(splits[0])\n","          labels.append(splits[-1])\n","          unique_labels.add(splits[-1])\n","      if words:\n","        examples.append({\"words\": words, \"labels\": labels})\n","    return examples, unique_labels\n","\n","  def convert_examples_to_inputs(self, examples):\n","    inputs = []\n","    for example in examples:\n","      inputs.append(\n","          self.convert_example_to_inputs(example[\"words\"], example[\"labels\"], self.label2id, self.max_seq_len, self.sep_token, \n","                                         self.cls_token, self.pad_token, self.ignore_label_id)\n","      )\n","\n","    return inputs\n","\n","  @staticmethod\n","  def convert_example_to_inputs(words, labels, label2id, max_seq_len=512, sep_token=\"[SEP]\", \n","                                cls_token=\"[PAD]\", pad_token=\"[PAD]\", ignore_index=-100):\n","    tokens, label_ids = [], []\n","    for i, (word, label) in enumerate(zip(words, labels)):\n","      word_tokens = tokenizer.tokenize(word)\n","      if len(word_tokens) > 0:\n","        tokens.extend(word_tokens)\n","        label_ids.extend([label2id[label]] + [ignore_index] * (len(word_tokens) - 1))\n","\n","    # truncate sentence if it's too long:\n","    if len(tokens) > max_seq_len - 2:  # 2, because we need to add CLS and SEP tokens\n","      tokens = tokens[:(max_seq_len - 2)]\n","      label_ids = label_ids[:(max_seq_len - 2)]\n","\n","    # adding the separator to the end of the sentence\n","    tokens += [sep_token]\n","    label_ids += [ignore_index]\n","\n","    # adding the classifier token to the start of the sentence\n","    tokens = [cls_token] + tokens\n","    label_ids = [ignore_index] + label_ids\n","\n","    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n","    input_mask = [1] * len(tokens)\n","\n","    # padding\n","    padding_length = max_seq_len - len(tokens)\n","    tokens += [pad_token] * padding_length\n","    label_ids += [ignore_index] * padding_length\n","    input_mask += [0] * padding_length\n","\n","    # convert tokens to input ids\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    return {\"input_ids\": input_ids, \"labels\": label_ids, \"attention_mask\": input_mask}\n","\n","# used to combine inputs into a batch\n","def collate_batch_to_tensors(inputs):\n","  batch = {}\n","  for k in inputs[0].keys():\n","    batch[k] = torch.tensor([dat[k] for dat in inputs], dtype=torch.long)\n","  \n","  return batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2grT0XQ20IBq"},"source":["## Model\n","\n","We've now implemented everything needed for the data side of the pipeline, let's now look at our model. The simplicity of using BERT for most of downstream tasks lies in the fact that we can just add a classification layer on top of produced representations and achieve good performance. To fine-tune the obtained model we update the combined parameters of both BERT and classification layer on downstream dataset.\n","\n","For the purposes of this workshop, we won't directly work with BERT model as we are constrained by computational power and time. We rather opt out for [DistilBERT](https://arxiv.org/abs/1910.01108). DistilBERT is a smaller version of BERT (same architecture, less layers) that is trained by distilling the knowledge of BERT to the new model. It is much faster and retains most of the representational power of BERT base model, so it's perfect for our use case.\n","\n","Named entity recogntion is a token classification task and a special version of DistillBERT for this type of downstream tasks is already implemented in *transformers* library, called [*DistilBertForTokenClassification*](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertfortokenclassificationps://). For demonstrational purposes we reimplement a DistillBERT with a classification head in the cell below."]},{"cell_type":"code","metadata":{"id":"uZv72PYjo_9t"},"source":["class DistilBertTokenClassificationModel(nn.Module):\n","  def __init__(self, bert_config, num_classes, dropout_prob=0.1):\n","    super(DistilBertTokenClassificationModel, self).__init__()\n","    self.num_classes = num_classes\n","\n","    self.bert = DistilBertModel(bert_config)\n","    self.dropout = nn.Dropout(dropout_prob)\n","    self.classification_layer = nn.Linear(in_features=bert_config.hidden_size, out_features=num_classes)\n","\n","  def forward(self, input_ids=None, attention_mask=None, labels=None):\n","    x = self.bert(input_ids, attention_mask)[0]  # produces token representations\n","    x = self.dropout(x)  # mitigates overfitting\n","    logits = self.classification_layer(x)  # classifies tokens into entity types\n","\n","    if labels is None:\n","      return logits\n","\n","    # compute the loss\n","    loss = nn.CrossEntropyLoss()(logits.view(-1, self.num_classes), labels.view(-1))\n","\n","    return (loss, logits)\n","\n","  def load(self, path_to_dir):\n","    self.bert = DistilBertModel.from_pretrained(path_to_dir)\n","    model_path = os.path.join(path_to_dir, \"model.tar\")\n","    if os.path.exists(model_path):\n","      checkpoint = torch.load(model_path)\n","      self.dropout.load_state_dict(checkpoint[\"dropout\"])\n","      self.classification_layer.load_state_dict(checkpoint[\"cls\"])\n","    else:\n","      print(\"No model.tar in provided directory, only loading bert model.\")\n","\n","  def save_pretrained(self, path_to_dir):\n","    self.bert.save_pretrained(path_to_dir)\n","    torch.save(\n","        {\"dropout\": self.dropout.state_dict(), \"cls\": self.classification_layer.state_dict()},\n","        os.path.join(path_to_dir, \"model.tar\")\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVJmjaSr3yTW"},"source":["Below we implement the Trainer class that contains the main train loop."]},{"cell_type":"code","metadata":{"id":"bHPQE8ampBnZ"},"source":["class Trainer:\n","  def __init__(self, model):\n","    self.model = model\n","\n","  def train(self, train_dataset, val_dataset, device, run_config):\n","    self.model = self.model.to(device)\n","    # create output folder if it doesn't yet exist\n","    if not os.path.isdir(run_config.output_dir): \n","      os.makedirs(run_config.output_dir)\n","    \n","    # train dataloader will serve us the training data in batches\n","    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), \n","                                  batch_size=run_config.batch_size, collate_fn=run_config.collate_fn)\n","    \n","    # optimizer and scheduler that modifies the learning rate during the training\n","    optimizer = AdamW(self.model.parameters(), lr=run_config.learning_rate)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=run_config.num_warmup_steps,\n","                                                num_training_steps=len(train_dataloader)*run_config.num_epochs)\n","    \n","    print(\"Training started:\")\n","    print(f\"\\tNum examples = {len(train_dataset)}\")\n","    print(f\"\\tNum Epochs = {run_config.num_epochs}\")\n","\n","    global_step = 0  # to save after every save_steps if save_steps is >= 0\n","\n","    train_iterator = trange(0, int(run_config.num_epochs), desc=\"Epoch\")\n","    for epoch in train_iterator:\n","      epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", position=0, leave=True)\n","      self.model.train()\n","      epoch_losses = []\n","      for step, inputs in enumerate(epoch_iterator):\n","        # move batch to GPU\n","        if isinstance(inputs, dict):\n","            for k, v in inputs.items():\n","                inputs[k] = v.to(device)\n","        else:\n","            inputs = inputs.to(device)\n","\n","        # forward pass - model also outputs a computed loss\n","        outputs = self.model(**inputs)\n","        loss = outputs[0]\n","\n","        epoch_losses.append(loss.item())\n","\n","        # backward pass - backpropagation\n","        self.model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        epoch_iterator.set_description(f\"Training loss = {loss.item():.4f}\")\n","\n","        if run_config.save_steps > -1 and global_step > 0 and global_step % run_config.save_steps == 0:\n","          output_dir = os.path.join(run_config.output_dir, f\"Step_{step}\")\n","          self.model.save_pretrained(output_dir)\n","          test_loss = self.evaluate(self.model, val_dataset, device, run_config)\n","          print(f\"After step {step + 1}: val loss ={test_loss}\")\n","\n","        global_step += 1\n","      \n","      if run_config.save_each_epoch:\n","        output_dir = os.path.join(run_config.output_dir, f\"Epoch_{epoch + 1}\")\n","        model.save_pretrained(output_dir)\n","        test_loss = self.evaluate(self.model, val_dataset, device, run_config)\n","        print(f\"After epoch {epoch + 1}: val loss ={test_loss}\")\n","\n","  def evaluate(self, model, test_dataset, device, run_config):\n","    test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset),\n","                                 batch_size=run_config.batch_size, collate_fn=run_config.collate_fn)\n","    self.model.eval()\n","    losses = []\n","    for inputs in tqdm(test_dataloader, desc=\"Evaluating\", position=0, leave=True):\n","      # move batch to GPU\n","      if isinstance(inputs, dict):\n","        for k, v in inputs.items():\n","          inputs[k] = v.to(device)\n","      else:\n","        inputs = inputs.to(device)\n","\n","      with torch.no_grad():\n","        loss = model(**inputs)[0]\n","      losses.append(loss.item())\n","\n","    return np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cFSlBOJU36cB"},"source":["*RunConfig* holds the parameter for training/testing:"]},{"cell_type":"code","metadata":{"id":"Js1jEfbGpDN1"},"source":["@dataclass\n","class RunConfig:\n","  learning_rate: float\n","  batch_size: int\n","  num_epochs: int\n","  num_warmup_steps: int = 1\n","  save_steps: int = -1\n","  save_each_epoch: bool = True\n","  output_dir: str = \"/content/\"\n","  collate_fn: None = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ncALuE6Q38q6"},"source":["## Training\n","\n","We have now implemented everything to start fine-tuning. We can save the fine-tuned models to our Colab instance (available under `/content/`) or we can connect our Google Drive to Colab and use it as external memory. If you want to do the latter, run the cell below and follow instructions."]},{"cell_type":"code","metadata":{"id":"FrRbAfLYpE5C"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jN5SYLKD4aRI"},"source":["Let's set the training parameters:"]},{"cell_type":"code","metadata":{"id":"QsYmHzWCpGK3"},"source":["run_config = RunConfig(\n","    learning_rate = 3e-5,\n","    batch_size = 32,  # start with 32 and decrease if you get CUDA out of memory exception\n","    num_epochs = 3,\n","    save_each_epoch = True,\n","    output_dir = \"/content/drive/My Drive/NLP-workshop/BERT-NER/\",\n","    collate_fn = collate_batch_to_tensors\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikFEoRYT4pIq"},"source":["Instantiating the datasets and tokenizer."]},{"cell_type":"code","metadata":{"id":"sUqe4rHApIzT"},"source":["max_len = 512  # max length of input, pretrained model only supports max_len up to 512, use smaller values for faster training\n","\n","# instantiate a DistillBert tokenizer\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n","\n","# datasets\n","train_dataset = TokenClassificationDataset(\"/content/conllpp_train.txt\", max_len)\n","val_dataset = TokenClassificationDataset(\"/content/conllpp_dev.txt\", max_len,\n","                                         train_dataset.label2id, train_dataset.id2label)\n","\n","if not os.path.isdir(run_config.output_dir): \n","  os.makedirs(run_config.output_dir)\n","\n","# we save label2id, id2label to a pickle file for later use\n","with open(os.path.join(run_config.output_dir, \"label2id.pkl\"), \"wb\") as f:\n","  pickle.dump(train_dataset.label2id, f, pickle.HIGHEST_PROTOCOL)\n","\n","with open(os.path.join(run_config.output_dir, \"id2label.pkl\"), \"wb\") as f:\n","  pickle.dump(train_dataset.id2label, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9AgLbNK40Qf"},"source":["Instatiate the model and start training!"]},{"cell_type":"code","metadata":{"id":"mcvDv1JxpKOQ"},"source":["model = DistilBertTokenClassificationModel(DistilBertConfig.from_pretrained(\"distilbert-base-uncased\"), \n","                                           num_classes=len(train_dataset.unique_labels))\n","model.load(\"distilbert-base-uncased\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jdzy7a0pLiQ"},"source":["trainer = Trainer(model)\n","trainer.train(train_dataset, val_dataset, device, run_config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjaw2rAW5VHA"},"source":["If you happen to get a CUDA out of memory exception, do the following:\n","- cause another exception so python doesn't hold any references to trainer or model, e.g. run the bottom cell causing ZeroDivisionError\n","- run the cell below that empties GPU cache\n","- decrease the batch_size in run_config and rerun that cell\n","- reinstantiate the model and rerun training"]},{"cell_type":"code","metadata":{"id":"kXBNa99RpMx1"},"source":["1 / 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glVXiIQEpN8K"},"source":["model = None\n","trainer = None\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GOduwhj20PnO"},"source":["## Evaluation\n","\n","After fine-tuning we have a BERT model specialized for detecting named entities in text. Let's see how it performs on the test set. For the purposes of this workshop we've prepared a pretrained model. You can get all the necessary files for evaluation by running the cell below."]},{"cell_type":"code","metadata":{"id":"QPWE_PAXpSDN"},"source":["!mkdir /content/bert-ner\n","!gdown -O /content/nertest.txt https://drive.google.com/uc?id=1q9EEadm5lBfWWOM8mITE3YJvhxbhNczC\n","!gdown https://drive.google.com/uc?id=10itTQ54hZd7G4t66KomAcjyMbfNJSSrm\n","!gdown https://drive.google.com/uc?id=10hEbs1zAeOBcG-wNLgSBS4iO3Aby2YoX\n","!gdown -O /content/bert-ner/config.json https://drive.google.com/uc?id=1-kUkp3AV4nfgjEuG2B88m_G2Gqc3g2bw\n","!gdown -O /content/bert-ner/model.tar https://drive.google.com/uc?id=1-o6aUye7n54EUER2B8V_Ue8Fbp4kUCSL\n","!gdown -O /content/bert-ner/pytorch_model.bin https://drive.google.com/uc?id=1-koeiI83JqeTdm0jH5zDRUmJfs9st8rJ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XD7Ein_j1PMJ"},"source":["Let's instantiate the dataset, tokenizer, model and reload label2id, id2label."]},{"cell_type":"code","metadata":{"id":"jhzQpWsWpU-1"},"source":["test_dataset = TokenClassificationDataset(\"/content/nertest.txt\", 512, label2id, id2label)\n","\n","with open(\"/content/label2id.pkl\", \"rb\") as f:\n","  label2id = pickle.load(f)\n","\n","with open(\"/content/id2label.pkl\", \"rb\") as f:\n","  id2label = pickle.load(f)\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = DistilBertTokenClassificationModel(DistilBertConfig.from_pretrained(\"distilbert-base-uncased\"), num_classes=len(id2label))\n","model.load(\"/content/bert-ner/\")\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jj6Nm4RE1bjr"},"source":["The following code is used to compute log loss and F1-score. As per the official BERT paper, we only consider the leading subword of each word for the label."]},{"cell_type":"code","metadata":{"id":"4olxNv7KpWi-"},"source":["def align_predictions_and_labels(predictions, labels):\n","  label_list = [[] for _ in range(labels.shape[0])]\n","  preds_list = [[] for _ in range(labels.shape[0])]\n","\n","  for i in range(labels.shape[0]):\n","    for j in range(labels.shape[1]):\n","      if labels[i, j] >= 0:  # otherwise, we ignore it\n","        label_list[i].append(id2label[labels[i][j]])\n","        preds_list[i].append(id2label[predictions[i][j]])\n","  return preds_list, label_list\n","\n","def evaluate(model, test_dataset, device, run_config):\n","  test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset),\n","                                batch_size=run_config.batch_size, collate_fn=run_config.collate_fn)\n","  model.eval()\n","  losses = []\n","  predictions, gt_labels = None, None\n","  for inputs in tqdm(test_dataloader, desc=\"Evaluating\", position=0, leave=True):\n","    # move batch to GPU\n","    for k, v in inputs.items():\n","      inputs[k] = v.to(device)\n","    \n","    with torch.no_grad():\n","      loss, logits = model(**inputs)\n","    losses.append(loss.item())\n","    if gt_labels is None:\n","      gt_labels = inputs[\"labels\"].detach().cpu().numpy()\n","      predictions = logits.detach().cpu().numpy()\n","    else:\n","      gt_labels = np.append(gt_labels, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n","      predictions = np.append(predictions, logits.detach().cpu().numpy(), axis=0)\n","\n","  predictions = np.argmax(predictions, axis=2)\n","  predictions, labels = align_predictions_and_labels(predictions, gt_labels)\n","\n","  f1 = f1_score(labels, predictions)\n","\n","  return np.mean(losses), f1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5AZ8iluhpYkq"},"source":["logloss, f1 = evaluate(model, test_dataset, device, run_config)\n","print(f\"\\nTest log loss = {logloss:.4f}\\nTest F1-score = {f1:.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KV5xlxVu11Jn"},"source":["With a F1-score of 0.90 we achieve quite okay performance. Let's now evaluate our model on some extra data, we've selected some BBC articles for this, but you're free to experiment!\n","\n","Sources for articles:\n","\n","- https://www.bbc.com/sport/formula1/54316085\n","- https://www.bbc.com/news/entertainment-arts-54292947\n","- http://www.bbc.com/travel/story/20200914-in-guatemala-the-maya-world-untouched-for-centuries"]},{"cell_type":"code","metadata":{"id":"NDvhlpXxpcRK"},"source":["label2color = {\n","    \"B-PER\" : \"red\",\n","    \"I-PER\" : \"red\",\n","    \"B-ORG\" : \"blue\",\n","    \"I-ORG\" : \"blue\",\n","    \"B-LOC\" : \"green\",\n","    \"I-LOC\" : \"green\",\n","    \"B-MISC\" : \"yellow\",\n","    \"I-MISC\" : \"yellow\",\n","    \"O\" : \"white\"\n","}\n","\n","def tag_some_text(text, show_legend=True):\n","  if show_legend:\n","    print(\"Legend: \" + colored(\"Person \", \"red\") + colored(\"Organization \", \"blue\") + \n","          colored(\"Location \", \"green\") + colored(\"Misc. \", \"yellow\") + \"\\n\\n\")\n","  words = text.split()\n","  fake_labels = [\"O\"] * len(text)\n","  inputs = TokenClassificationDataset.convert_example_to_inputs(words, fake_labels, label2id)\n","  for k, v in inputs.items():\n","    input_ids = torch.tensor([inputs[\"input_ids\"]], dtype=torch.long).to(device)\n","    attention_mask = torch.tensor([inputs[\"attention_mask\"]], dtype=torch.long).to(device)\n","      \n","  with torch.no_grad():\n","    logits = model(input_ids, attention_mask)\n","  predictions = np.argmax(logits.cpu().numpy(), axis=2)\n","  predictions, _ = align_predictions_and_labels(predictions, np.array([inputs[\"labels\"]]))\n","  colors = list(map(label2color.get, predictions[0]))\n","  colored_words = []\n","  for i in range(len(words)):\n","    colored_words.append(colored(words[i], colors[i])) \n","  print(\" \".join(colored_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8pTMD1-fpnDs"},"source":["tag_some_text(\"Lewis Hamilton's quest for the all-time record of Formula 1 wins was put on hold when he was hit with penalties at the Russian Grand Prix. Hamilton's Mercedes team-mate Valtteri Bottas dominated after the world champion was given a 10-second penalty for doing two illegal practice starts. Bottas was on the better strategy - starting on the medium tyres while Hamilton was on softs after a chaotic qualifying session for the Briton - and was tracking Hamilton in the early laps waiting for the race to play out. Behind the top three, Racing Point's Sergio Perez and Renault's Daniel Ricciardo had equally lonely races, the Australian having sufficient pace to overcome a five-second penalty for failing to comply with rules regarding how to rejoin the track when a car runs wide at Turn Two. Ferrari's Charles Leclerc made excellent use of a long first stint on the medium tyres to vault up from 11th on the grid to finish sixth, ahead of the second Renault of Esteban Ocon, the Alpha Tauris of Daniil Kvyat and Pierre Gasly and Alexander Albon's Red Bull. What's next? The Eifel Grand Prix on 11 October as the Nurburgring returns to the F1 calendar for the first time since 2013. The 24-hour touring car race there this weekend has been hit with miserable wet and wintery conditions in the Eifel mountains. Will F1 face the same?\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MroHaWlfpovt"},"source":["tag_some_text(\"Sir David Attenborough has broken Jennifer Aniston's record for the fastest time to reach a million followers on Instagram. At 94 years young, the naturalist's follower count raced to seven figures in four hours 44 minutes on Thursday, according to Guinness World Records. His debut post said: \\'Saving our planet is now a communications challenge.\\' Last October, Friends star Aniston reached the milestone in five hours and 16 minutes. Sir David's Instagram debut precedes the release of a book and a Netflix documentary, both titled A Life On Our Planet.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"esoMavJ5pqJ7"},"source":["tag_some_text(\"Using Lidar, in 2016 the Foundation for Maya Cultural and Natural Heritage launched the largest archaeological survey ever undertaken of the Maya lowlands. In the first phase, whose results were published in 2018, they mapped 2,100km of the Maya Biosphere Reserve. Their hope in the further phases – the second one of which took place in summer 2019, while I was there – is to triple the coverage area. That would make the project the largest Lidar survey not only in Central America, but in the world.\")"],"execution_count":null,"outputs":[]}]}