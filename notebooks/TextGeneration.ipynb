{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gpt_text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LurZsC5XmjQ3"
      },
      "source": [
        "# Text Generation - Fine-tuning GPT-2\n",
        "\n",
        "In this notebook we'll tackle the task of text generation with the notorious [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model. We'll look at data preparation and fine-tuning process needed in order for GPT-2 to produce desired text. Our goal in this notebook is to fine-tune a pretrained model to produce motivational/inspirational quotes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZeHU6gtMz2M"
      },
      "source": [
        "First things first, let's make sure we have a GPU instance in this Colab session:\n",
        "- `Edit -> Notebook settings -> Hardware accelerator` must be set to GPU\n",
        "- if needed, reinitiliaze the session by clicking `Connect` in top right corner\n",
        "\n",
        "After the session is initilized, we can check our assigned GPU with the following command (fingers crossed it's a Tesla P100 :P):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWp4dQM5WLuA"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zj4tDXwOWdG"
      },
      "source": [
        "Let's install and import everything we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4g19tqPGS-G"
      },
      "source": [
        "!wget https://github.com/andrejmiscic/NLP-workshop/raw/master/utils/generation_utils.py\r\n",
        "!wget https://github.com/andrejmiscic/NLP-workshop/raw/master/utils/trainer.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFPSoj9aWOwb"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn4suPNeWQTc"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from generation_utils import TextDatasetWithEpochs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from trainer import Trainer, RunConfig\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, DataCollatorForLanguageModeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThSJpQazW-Cl"
      },
      "source": [
        "## Data\n",
        "\n",
        "As we already mentioned, our goal is to fine-tune a GPT-2 model to produce motivational quotes. For this reason we are working with a dataset of 32716 quotes collected and cleaned from this [Kaggle source](https://www.kaggle.com/stuffbyyc/quotes). \n",
        "\n",
        "Let's load the data and take a look at some of the examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gBrkhVLWYFz"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/MotivationalQuotes/quotes.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhJbCmB8WZ0W"
      },
      "source": [
        "with open(\"/content/quotes.txt\", \"r\") as f:\n",
        "    examples = [l.strip() for l in f.readlines()]\n",
        "\n",
        "print(f\"Our dataset contains {len(examples)} quotes.\")\n",
        "print(f\"Some examples:\")\n",
        "print(\"- \" + \"\\n- \".join(examples[:3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyerWXPYeRr4"
      },
      "source": [
        "Good, after looking at some of these quotes we're of course super motivated to continue. Our next step is to design a dataset class that will hold our data and serve training samples.\n",
        "\n",
        "Input to the GPT model is a batch of lists of input ids, these are the indices of tokens in the vocabulary. In order to form a batch, these lists must be of equal length. We usually achieve this by truncating too long quotes and padding too short ones. However, here we opt for a different approach:\n",
        "\n",
        "First we combine all the of quotes into one long text - a motivational essay if you will :). To delimit each of the samples we introduce a new token - \"example delimeter\" `<|endoftext|>`. To create training samples from this motivational essay, we \"cut\" it into blocks of tokens (each block is of predefined size - `block_size`). One problem remains, if we leave the quotes in the same order as they are in the dataset, the model might pickup certain dependencies between consecutive quotes that we wouldn't like it to learn. To mitigate this we create a dataset that already has combined data for all of required training epochs - and we shuffle the quotes for each epoch.\n",
        "\n",
        "Finally, there is another reason to include the delimeter `<|endoftext|>`. The model will learn to produce a quote after it and also end the quote with it. Therefore we can prompt a fine-tuned model with `<|endoftext|>` to produce a new motivational quote.\n",
        "\n",
        "We've implemented these dataset structure in `generation_utils.py` and named it *TextDatasetWithEpochs*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuyRieicNxhk"
      },
      "source": [
        "block_size = 128  # length of input samples\r\n",
        "num_dataset_epochs = 8  # used to create the dataset, during training we'll only use 1 epoch\r\n",
        "\r\n",
        "train_examples, valid_examples = train_test_split(examples, test_size=0.2)\r\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\r\n",
        "\r\n",
        "train_dataset = TextDatasetWithEpochs(train_examples, tokenizer, block_size, num_dataset_epochs)\r\n",
        "val_dataset = TextDatasetWithEpochs(valid_examples, tokenizer, block_size, 1)\r\n",
        "collate_call = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCkx8cHQznNV"
      },
      "source": [
        "## Training\n",
        "\n",
        "We have now implemented everything we need to start fine-tuning. We can save the fine-tuned models to our Colab instance (available under `/content/`) or we can connect our Google Drive to Colab and use it as external memory. If you want to do the latter, run the cell below and follow instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyJeCVJkWhcJ"
      },
      "source": [
        "# optional if you want to save your models to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbJXEnFS0rQU"
      },
      "source": [
        "Google Drive is now accessable under `/content/drive/`.\n",
        "\n",
        "Let's set the training parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLV4CmPqWlpN"
      },
      "source": [
        "run_config = RunConfig(\n",
        "    learning_rate = 3e-5,\n",
        "    batch_size = 32,  # start with 32 and decrease if you get CUDA out of memory exception\n",
        "    num_epochs = 1,  # the dataset already encodes the epochs\n",
        "    output_dir = \"/content/drive/MyDrive/NLP-workshop-materials/GPT2-generation/\",\n",
        "    collate_fn = collate_call\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w659un9U6MFi"
      },
      "source": [
        "Let's instantiate the pretrained GPT-2 model. We are using the small version of GPT-2 with 12 layers, 768 hidden dimension, 12 attention heads which combines for 117M parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoBPiitnWnhF"
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxSCfeA9Wspz"
      },
      "source": [
        "trainer = Trainer(model)\n",
        "trainer.train(train_dataset, val_dataset, device, run_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8KhjhlTAgKM"
      },
      "source": [
        "If you happen to get a CUDA out of memory exception, do the following:\n",
        "- cause another exception so python doesn't hold any references to trainer or model, e.g. run the bottom cell causing ZeroDivisionError\n",
        "- run the cell below that empties GPU cache\n",
        "- decrease the batch_size in run_config and rerun that cell\n",
        "- reinstantiate the model and rerun training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuIJuYDvWuW5"
      },
      "source": [
        "1 / 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBkWNvfTWv4B"
      },
      "source": [
        "model = None\n",
        "trainer = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoPifccwCRtQ"
      },
      "source": [
        "For the purposes of this workshop we already fine-tuned a GPT-2 model, let's load it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5NpZZ_6BxMl"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We now have a fine-tuned GPT-2 model ready to produce motivational quotes. GPT-2 outputs a probability distribution over the next token conditioned on previous ones. There are a couple of ways we can go about generating text:\n",
        "- Greedy decoding\n",
        "- Beam search\n",
        "- Top-k/Top-p sampling\n",
        "\n",
        "You can read more [here](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "Let's first download and initilize the already fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7135v1sPTzF"
      },
      "source": [
        "!mkdir /content/gpt2-quotes\r\n",
        "!gdown -O /content/gpt2-quotes/config.json https://drive.google.com/uc?id=1-AYqNe0968Ru-m4qXwbyVPHyjf9cJIWc\r\n",
        "!gdown -O /content/gpt2-quotes/pytorch_model.bin https://drive.google.com/uc?id=1-CpfjekRPQX_FWt5FQzkv81GdYIsgr4M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrU2UnzYPVAZ"
      },
      "source": [
        "# only run if you want to use the model we've already fine-tuned for you\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\r\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/gpt2-quotes/\").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNxd_mGVUe6o"
      },
      "source": [
        "#### Greedy decoding\n",
        "This is the simplest approach, at every step we just select the most probable next word, i.e. the word with highest outputed probability. One can immediately see that after some text the model will start repeating itself. This would therefore be a bad decoding scheme if we want to produce long continuous text, but since we're producing fairly short quotes it might achieve okay results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D3vNdVRVv0j"
      },
      "source": [
        "<div>\r\n",
        "<img src=\"https://github.com/andrejmiscic/NLP-workshop/raw/master/figures/greedy.PNG\" width=\"800\"/>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjRjJt_8W1Lc"
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"transformers.generation_utils\").setLevel(logging.CRITICAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xK-k5zYW2pc"
      },
      "source": [
        "def generate_text_greedy(prompt=\"\", max_length=64):\n",
        "  model.eval()\n",
        "  input_ids = tokenizer.encode(\"<|endoftext|>\" + prompt, return_tensors='pt').to(device)\n",
        "  generated_ids = model.generate(input_ids, max_length=max_length).cpu().tolist()\n",
        "\n",
        "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "  return generated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suVEaNXaW4Or"
      },
      "source": [
        "generate_text_greedy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2BKZDfwGRqh"
      },
      "source": [
        "Wow, so deep. Since greedy decoding is deterministic, this is the only quote produced by default prompt, but we can initialize the prompt with some text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mGfrYgkW6L4"
      },
      "source": [
        "print(generate_text_greedy(\"I believe\"))\n",
        "print(generate_text_greedy(\"Data science\"))\n",
        "print(generate_text_greedy(\"Just\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdRBlBHsIJgp"
      },
      "source": [
        "#### Beam search\n",
        "\n",
        "Beam search is also a deterministic decoding, but offers an improvement over greedy decoding. A problem of greedy decoding is that we might miss the most likely sequence since we predict only the most probable word at each timestep. Beam search mitigates this by keeping a track of most probable *n* sequences at every step and ultimately selecting the most probable sequence.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/andrejmiscic/NLP-workshop/raw/master/figures/beam.PNG\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1w1p4WgXBNO"
      },
      "source": [
        "def generate_text_beam(prompt=\"\", max_length=64, num_beams=8):\n",
        "  model.eval()\n",
        "  input_ids = tokenizer.encode(\"<|endoftext|>\" + prompt, return_tensors='pt').cuda()\n",
        "  generated_ids = model.generate(input_ids, max_length=max_length, num_beams=num_beams,\n",
        "                                 no_repeat_ngram_size=2).cpu().tolist()\n",
        "\n",
        "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "  return generated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TacICaPuXEH5"
      },
      "source": [
        "generate_text_beam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5loQbjVaXGjn"
      },
      "source": [
        "print(generate_text_beam(\"I believe\"))\n",
        "print(generate_text_beam(\"Data science\"))\n",
        "print(generate_text_beam(\"Just\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5rvB4XWJflA"
      },
      "source": [
        "#### Top-k/Top-p sampling\n",
        "\n",
        "We've looked at two deterministic decoding schemes, let's now focus on non-deterministic that is based on sampling the next word from a probability distribution. Outputed probability distribution is over the entire model vocabulary (order of tens of thousands), it has most of its mass on a subset of most probable words and a very long tail. The tokens in the tail part would produce incoherent gibberish therefore we must somehow limit ourselves to only sample from most probable words. That's where top-k and top-p sampling come into play:\n",
        "\n",
        "- [Top-k sampling](https://arxiv.org/abs/1805.04833) selects *k* most probable words and distributes their comulative probability over them. The problem is that we must choose a fixed sized parameter *k* which might lead to suboptimal results in some scenarios.\n",
        "- [Top-p sampling](https://arxiv.org/abs/1904.09751) addresses this by selecting top words whose cumulative probability just exceeds p. This comulative probability is then again distributed among these words.\n",
        "\n",
        "We'll use a combination of both in this notebook, but you're free to test different scenarios.\n",
        "\n",
        "There is another parameter that we haven't introduced: `temperature` which controls the outputed distribution from softmax function. Regular softmax has `temperature` = 1. If `temperature` -> 0, we give more probability mass to more probable words (we go towards greedy decoding). Higher values cause a more uniform distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Oj740svm0N"
      },
      "source": [
        "<div>\r\n",
        "<img src=\"https://github.com/andrejmiscic/NLP-workshop/raw/master/figures/topk.PNG\" width=\"800\"/>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6EY1wrAXITB"
      },
      "source": [
        "def generate_text_sampling(prompt=\"\", max_length=64, top_k=50, top_p=0.95, temp=1.0, num_return=1):\n",
        "  model.eval()\n",
        "  input_ids = tokenizer.encode(\"<|endoftext|>\" + prompt, return_tensors='pt').cuda()\n",
        "  generated_ids = model.generate(input_ids, do_sample=True, max_length=max_length, temperature=temp, \n",
        "                                 top_k=top_k, top_p=top_p, num_return_sequences=num_return).cpu().tolist()\n",
        "\n",
        "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "  return generated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chYFL3fUXJ5k"
      },
      "source": [
        "generate_text_sampling(num_return=3, temp=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVQGxek7XL4q"
      },
      "source": [
        "print(generate_text_sampling(\"I believe\", num_return=3, temp=0.7))\n",
        "print(generate_text_sampling(\"Data science\", num_return=3, temp=0.7))\n",
        "print(generate_text_sampling(\"Just\", num_return=3, temp=0.7))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}