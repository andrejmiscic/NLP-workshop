{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"imdb.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPJTGRlIH0ZHi/1B4nVWY0k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"aSobi1Ca4i0W"},"source":["# Sentiment analysis - fine-tuning BERT\n","\n","In this notebook we'll take a look at the process needed to fine-tine a pretrained [BERT](https://arxiv.org/abs/1810.04805) model to detect sentiment of a piece of text. Our goal will be to classify the polarity of IMDB movie reviews. For this purpose we'll be working with a dataset from this [Kaggle source](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/notebooks). The techniques we'll discuss also apply to text classification."]},{"cell_type":"markdown","metadata":{"id":"TKH-fJ4BqHmB"},"source":["First things first, let's make sure we have a GPU instance in this Colab session:\n","- `Edit -> Notebook settings -> Hardware accelerator` must be set to GPU\n","- if needed, reinitiliaze the session by clicking `Connect` in top right corner\n","\n","After the session is initilized, we can check our assigned GPU with the following command (fingers crossed it's a Tesla P100!!):"]},{"cell_type":"code","metadata":{"id":"7y-rUMHAnwBZ"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bMI65N3GGnkr"},"source":["Let's install some additional libraries: *transformers* for BERT implementation and *gdown* for loading from Drive."]},{"cell_type":"code","metadata":{"id":"777JEt-8nyLi"},"source":["!pip install transformers\n","!pip install gdown"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jos1ATjtnyur"},"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.tensorboard import SummaryWriter\n","from transformers import BertModel, BertConfig, BertTokenizerFast, get_linear_schedule_with_warmup, AdamW\n","from transformers import DistilBertConfig, DistilBertModel, DistilBertTokenizerFast\n","from sklearn.model_selection import train_test_split\n","import os\n","from tqdm import tqdm, trange\n","import numpy as np\n","import pandas as pd\n","from dataclasses import dataclass\n","import gc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GaUnEp2fbQWp"},"source":["## Data\n","\n","Let's take a look at our dataset of IMDB reviews:"]},{"cell_type":"code","metadata":{"id":"Ten6sF7vn55k"},"source":["path_to_train_csv = \"https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/imdb_train.csv\"\n","\n","df = pd.read_csv(path_to_train_csv)\n","pd.set_option('display.max_colwidth', -1)\n","print(df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PigdQ0EHc4aL"},"source":["Notice that our reviews fall into two polarity classes: *positive* and *negative*. This is therefore a binary sequence classification task. Below we implement a dataset holder and a collate function to combine the batch."]},{"cell_type":"code","metadata":{"id":"LKpx0xIEn8fN"},"source":["class TextClassificationDataset(Dataset):\n","    def __init__(self, inputs, labels, tokenizer, max_len):\n","        super(TextClassificationDataset, self).__init__()\n","\n","        # encodes the inputs to input_ids and attention_mask\n","        encoded_inputs = tokenizer(inputs, max_length=max_len, padding=\"max_length\", truncation=True)\n","        self.data = list(zip(encoded_inputs[\"input_ids\"], encoded_inputs[\"attention_mask\"], labels))\n","\n","    def __getitem__(self, i):\n","        return self.data[i]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","def collate_batch_to_tensors(inputs):\n","    batch = {\"input_ids\": torch.tensor([dat[0] for dat in inputs], dtype=torch.long),\n","            \"attention_mask\": torch.tensor([dat[1] for dat in inputs], dtype=torch.long),\n","            \"labels\": torch.tensor([dat[2] for dat in inputs], dtype=torch.long)}\n","    return batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqGzX_qeeBC1"},"source":["## Model\n","\n","Same as for Named Entity Recognition we are working with DistilBERT, a smaller model than base BERT, that is though by knowledge distillation and retains most of the performance. As mentioned during the lectures, BERT has a special token (*CLS*) whose representation we use as inputs to a classifier. During pretraining this is trained on the task of next sentence prediction therefore out of the box it is not useful as sequence representation. That's where finetuning comes in - we train a classifier together with pretrained BERT model to achieve good performance.\n","\n","An architecture for sequence classification is already impemented in *transformers* library: [*DistilBertForSequenceClassification*](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification), but for demonstrational purposes we reimplement a DistilBERT with a classification head below."]},{"cell_type":"code","metadata":{"id":"pVaOgbTloLnp"},"source":["class DistilBertTextClassificationModel(nn.Module):\n","    def __init__(self, bert_config, num_classes, dropout_prob=0.1):\n","        super(DistilBertTextClassificationModel, self).__init__()\n","\n","        self.bert = DistilBertModel(bert_config)\n","        self.dropout = nn.Dropout(dropout_prob)\n","        self.classification_layer = nn.Linear(in_features=bert_config.hidden_size, out_features=num_classes)\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        outputs = self.bert(input_ids, attention_mask)[0]\n","        cls = outputs[:, 0, :]  # cls is the first token of the sequence\n","        cls = self.dropout(cls)  # to mitigate overfitting\n","        logits = self.classification_layer(cls)  # classify\n","\n","        if labels is None:\n","          return logits\n","\n","        # compute the loss\n","        loss = nn.CrossEntropyLoss()(logits, labels)\n","\n","        return (loss, logits)\n","\n","    def load(self, path_to_dir):\n","        self.bert = DistilBertModel.from_pretrained(path_to_dir)\n","        model_path = os.path.join(path_to_dir, \"model.tar\")\n","        if os.path.exists(model_path):\n","            checkpoint = torch.load(model_path)\n","            self.dropout.load_state_dict(checkpoint[\"dropout\"])\n","            self.classification_layer.load_state_dict(checkpoint[\"cls\"])\n","        else:\n","            print(\"No model.tar in provided directory, only loading bert model.\")\n","\n","    def save_pretrained(self, path_to_dir):\n","        self.bert.save_pretrained(path_to_dir)\n","        torch.save(\n","            {\"dropout\": self.dropout.state_dict(), \"cls\": self.classification_layer.state_dict()},\n","            os.path.join(path_to_dir, \"model.tar\")\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xiGUqlvgk5Pj"},"source":["Below we implement the Trainer class that contains the main train loop."]},{"cell_type":"code","metadata":{"id":"L8MzkqT5oNR6"},"source":["class Trainer:\n","  def __init__(self, model):\n","    self.model = model\n","\n","  def train(self, train_dataset, val_dataset, device, run_config):\n","    self.model = self.model.to(device)\n","    # create output folder if it doesn't yet exist\n","    if not os.path.isdir(run_config.output_dir): \n","      os.makedirs(run_config.output_dir)\n","    \n","    # train dataloader will serve us the training data in batches\n","    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), \n","                                  batch_size=run_config.batch_size, collate_fn=run_config.collate_fn)\n","    \n","    # optimizer and scheduler that modifies the learning rate during the training\n","    optimizer = AdamW(self.model.parameters(), lr=run_config.learning_rate)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=run_config.num_warmup_steps,\n","                                                num_training_steps=len(train_dataloader)*run_config.num_epochs)\n","    \n","    print(\"Training started:\")\n","    print(f\"\\tNum examples = {len(train_dataset)}\")\n","    print(f\"\\tNum Epochs = {run_config.num_epochs}\")\n","\n","    global_step = 0  # to save after every save_steps if save_steps is >= 0\n","\n","    train_iterator = trange(0, int(run_config.num_epochs), desc=\"Epoch\")\n","    for epoch in train_iterator:\n","      epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", position=0, leave=True)\n","      self.model.train()\n","      epoch_losses = []\n","      for step, inputs in enumerate(epoch_iterator):\n","        # move batch to GPU\n","        if isinstance(inputs, dict):\n","            for k, v in inputs.items():\n","                inputs[k] = v.to(device)\n","        else:\n","            inputs = inputs.to(device)\n","\n","        # forward pass - model also outputs a computed loss\n","        outputs = self.model(**inputs)\n","        loss = outputs[0]\n","\n","        epoch_losses.append(loss.item())\n","\n","        # backward pass - backpropagation\n","        self.model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        epoch_iterator.set_description(f\"Training loss = {loss.item():.4f}\")\n","\n","        if run_config.save_steps > -1 and global_step > 0 and global_step % run_config.save_steps == 0:\n","          output_dir = os.path.join(run_config.output_dir, f\"Step_{step}\")\n","          self.model.save_pretrained(output_dir)\n","          test_loss = self.evaluate(self.model, val_dataset, device, run_config)\n","          print(f\"After step {step + 1}: val loss ={test_loss}\")\n","\n","        global_step += 1\n","      \n","      if run_config.save_each_epoch:\n","        output_dir = os.path.join(run_config.output_dir, f\"Epoch_{epoch + 1}\")\n","        model.save_pretrained(output_dir)\n","        test_loss = self.evaluate(self.model, val_dataset, device, run_config)\n","        print(f\"After epoch {epoch + 1}: val loss ={test_loss}\")\n","\n","  def evaluate(self, model, test_dataset, device, run_config):\n","    test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset),\n","                                 batch_size=run_config.batch_size, collate_fn=run_config.collate_fn)\n","    self.model.eval()\n","    losses = []\n","    for inputs in tqdm(test_dataloader, desc=\"Evaluating\", position=0, leave=True):\n","      # move batch to GPU\n","      if isinstance(inputs, dict):\n","        for k, v in inputs.items():\n","          inputs[k] = v.to(device)\n","      else:\n","        inputs = inputs.to(device)\n","\n","      with torch.no_grad():\n","        loss = model(**inputs)[0]\n","      losses.append(loss.item())\n","\n","    return np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVPwkY6dk9Ca"},"source":["*RunConfig* holds the parameter for training/testing:"]},{"cell_type":"code","metadata":{"id":"HbgVK1xBoO0x"},"source":["@dataclass\n","class RunConfig:\n","  learning_rate: float\n","  batch_size: int\n","  num_epochs: int\n","  num_warmup_steps: int = 1\n","  save_steps: int = -1\n","  save_each_epoch: bool = True\n","  output_dir: str = \"/content/\"\n","  collate_fn: None = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLDp9pJvk_IP"},"source":["## Training\n","\n","We have now implemented everything to start fine-tuning. We can save the fine-tuned models to our Colab instance (available under `/content/`) or we can connect our Google Drive to Colab and use it as external memory. If you want to do the latter, run the cell below and follow instructions."]},{"cell_type":"code","metadata":{"id":"8zPBX4qloQYz"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJTtkmhToRog"},"source":["run_config = RunConfig(\n","    learning_rate = 3e-5,\n","    batch_size = 4,  # start with 32 and decrease if you get CUDA out of memory exception\n","    num_epochs = 3,\n","    save_each_epoch = True,\n","    output_dir = \"/content/drive/My Drive/NLP-workshop/BERT-sentiment/\",\n","    collate_fn = collate_batch_to_tensors\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cl4pEBAaoSzP"},"source":["path_to_train_csv = \"https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/imdb_train.csv\"\n","\n","sentiment_to_label = {\"negative\": 0, \"positive\": 1}\n","label_to_sentiment = {0: \"negative\", 1: \"positive\"}\n","\n","df = pd.read_csv(path_to_train_csv)\n","df[\"label\"] = df[\"sentiment\"].map(sentiment_to_label)\n","train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CWww00ZoUFZ"},"source":["max_len = 512  # max length of input, pretrained model only supports max_len up to 512, use smaller values for faster training\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n","\n","train_dataset = TextClassificationDataset(train_df[\"review\"].tolist(), train_df[\"label\"].tolist(), tokenizer, max_len)\n","val_dataset = TextClassificationDataset(val_df[\"review\"].tolist(), val_df[\"label\"].tolist(), tokenizer, max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hfMcapBclKuR"},"source":["Instatiate the model and start training!"]},{"cell_type":"code","metadata":{"id":"JQTMCKohoVsO"},"source":["model = DistilBertTextClassificationModel(DistilBertConfig.from_pretrained(\"distilbert-base-uncased\"), num_classes=2)\n","model.load(\"distilbert-base-uncased\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbYnFUoroXMD"},"source":["trainer = Trainer(model)\n","trainer.train(train_dataset, val_dataset, device, run_config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NqHqkCUBlNIB"},"source":["If you happen to get a CUDA out of memory exception, do the following:\n","- cause another exception so python doesn't hold any references to trainer or model, e.g. run the bottom cell causing ZeroDivisionError\n","- run the cell below that empties GPU cache\n","- decrease the batch_size in run_config and rerun that cell\n","- reinstantiate the model and rerun training"]},{"cell_type":"code","metadata":{"id":"D8-VI7lSoY2h"},"source":["1/0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xib_4trnobL8"},"source":["model = None\n","trainer = None\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qsmORIJVlZLl"},"source":["## Evaluation\n","\n","With this procedure we've now fine-tuned a model to predict the polarity of a review. For the purposes of this workshop we've pretrained a model so we can analyze it. Load it by running the cell below."]},{"cell_type":"code","metadata":{"id":"30QoRzNOocjC"},"source":["!mkdir /content/bert-imdb\n","!gdown https://drive.google.com/uc?id=10itTQ54hZd7G4t66KomAcjyMbfNJSSrm\n","!gdown https://drive.google.com/uc?id=10hEbs1zAeOBcG-wNLgSBS4iO3Aby2YoX\n","!gdown -O /content/bert-imdb/config.json https://drive.google.com/uc?id=1-vFT2MCAep0MHKgPOjpwYin0HOOsVhPt\n","!gdown -O /content/bert-imdb/model.tar https://drive.google.com/uc?id=1-xMaX3dYJ2cal1LsQhlnex9jXpHWHIL8\n","!gdown -O /content/bert-imdb/pytorch_model.bin https://drive.google.com/uc?id=1-pklw-2XLo3IKbl59ybuZnqz2FwOYaDn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gdl_VTI9l-M0"},"source":["Let's instantiate all the object we need for evaluation: model, dataset, tokenizer, etc."]},{"cell_type":"code","metadata":{"id":"dE-gcCzCodv6"},"source":["path_to_test_csv = \"https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/imdb_test.csv\"\n","\n","sentiment_to_label = {\"negative\": 0, \"positive\": 1}\n","label_to_sentiment = {0: \"negative\", 1: \"positive\"}\n","\n","df = pd.read_csv(path_to_test_csv)\n","df[\"label\"] = df[\"sentiment\"].map(sentiment_to_label)\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n","test_dataset = TextClassificationDataset(df[\"review\"].tolist(), df[\"label\"].tolist(), tokenizer, 512)\n","\n","model = DistilBertTextClassificationModel(DistilBertConfig.from_pretrained(\"distilbert-base-uncased\"), num_classes=2)\n","model.load(\"/content/bert-imdb/\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mNGJ0UwHmKyC"},"source":["The following code will compute log loss and accuracy on the test set."]},{"cell_type":"code","metadata":{"id":"FifJuuapofJV"},"source":["def evaluate(model, test_dataset, device, run_config):\n","    test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset),\n","                                 batch_size=run_config.batch_size, collate_fn=run_config.collate_fn)\n","    model.eval()\n","    ce_losses, acc_losses = [], []\n","    with torch.no_grad():\n","      for inputs in tqdm(test_dataloader, desc=\"Evaluating\", position=0, leave=True):\n","          # move batch to GPU\n","          if isinstance(inputs, dict):\n","            for k, v in inputs.items():\n","              inputs[k] = v.to(device)\n","          else:\n","            inputs = inputs.to(device)\n","\n","          loss, logits_y = model(**inputs)\n","          ce_losses.append(loss.item())\n","          pred_y = np.argmax(nn.functional.softmax(logits_y, dim=1).squeeze().cpu().numpy(), axis=1)  # beautiful\n","          true_y = inputs[\"labels\"].cpu().numpy()\n","          acc_losses.append(np.mean(pred_y == true_y))\n","\n","    return np.mean(ce_losses), np.mean(acc_losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oqSWgtnFogiz"},"source":["log_loss, accuracy = evaluate(model, test_dataset, device, run_config)\n","print(f\"\\nTest log loss = {log_loss:.4f}\\nTest accuracy = {accuracy:.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7wYCI1Km_qi"},"source":["Nice, we achieve a relatively good accuracy of 0.93. We can now experiment with the model and write some custom reviews."]},{"cell_type":"code","metadata":{"id":"MtnluYjeoh5i"},"source":["def predict_review_sentiment(review: str):\n","  enc = tokenizer(review)\n","  inputs = {\"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device),\n","            \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)}\n","  with torch.no_grad():\n","    prediction = np.argmax(nn.functional.softmax(model(**inputs), dim=1).cpu().numpy())\n","  print(review)\n","  print(f\"Sentiment: {label_to_sentiment.get(prediction)}\")"],"execution_count":null,"outputs":[]}]}