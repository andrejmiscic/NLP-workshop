{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LurZsC5XmjQ3"
   },
   "source": [
    "# Text Generation - Fine-tuning GPT-2\n",
    "\n",
    "In this notebook we'll tackle the task of text generation with the notorious [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model. We'll look at data preparation and fine-tuning process needed in order for GPT-2 to produce desired text. Our goal in this notebook is to fine-tune a pretrained model to produce motivational/inspirational quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZeHU6gtMz2M"
   },
   "source": [
    "First things first, let's make sure we have a GPU instance in this Colab session:\n",
    "- `Edit -> Notebook settings -> Hardware accelerator` must be set to GPU\n",
    "- if needed, reinitiliaze the session by clicking `Connect` in top right corner\n",
    "\n",
    "After the session is initilized, we can check our assigned GPU with the following command (fingers crossed it's a Tesla P100 :P):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWp4dQM5WLuA"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zj4tDXwOWdG"
   },
   "source": [
    "Let's install the *transformers* library from [Huggingface](https://huggingface.co/) that we're using, *gdown* for loading files from Drive and import everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFPSoj9aWOwb"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn4suPNeWQTc"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import GPT2TokenizerFast, TextDataset, GPT2LMHeadModel, DataCollatorForLanguageModeling, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThSJpQazW-Cl"
   },
   "source": [
    "## Data\n",
    "\n",
    "As we already mentioned, our goal is to fine-tune a GPT-2 model to produce motivational quotes. For this reason we are working with a dataset of 32716 quotes collected and cleaned from this [Kaggle source](https://www.kaggle.com/stuffbyyc/quotes). \n",
    "\n",
    "Let's load the data and take a look at some of the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gBrkhVLWYFz"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/quotes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhJbCmB8WZ0W"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/quotes.txt\", \"r\") as f:\n",
    "    examples = [l.strip() for l in f.readlines()]\n",
    "\n",
    "print(f\"Our dataset contains {len(examples)} quotes.\")\n",
    "print(f\"Some examples:\")\n",
    "print(\"- \" + \"\\n- \".join(examples[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyerWXPYeRr4"
   },
   "source": [
    "Good, after looking at some of these quotes we're very motivated to continue. Our next step is to design a dataset class that will hold our data and serve training samples.\n",
    "\n",
    "Inputs to the GPT model are batches of same length input ids - indices of tokens in vocabulary (GPT uses [Byte Pair Encoding](https://arxiv.org/abs/1508.07909) input representation not WordPiece as BERT). Looking at our samples above we can notice that they differ in length quite a lot. Combining them into batches would require a lot of truncating and padding. We rather opt out for a different approach - we combine all the of quotes into one long text - a motivational essay if you will :). To delimit each of the samples we introduce a new token - example delimeter `<|endoftext|>`. To create training samples from this motivational essay, we \"cut\" it into blocks of tokens (each block is of predefined size - `block_size`). One problem remains, if we leave the quotes in the same order as they are in the dataset, the model might pickup certain dependencies between consecutive quotes that we wouldn't like it to learn. To mitigate this we create a dataset that already has combined data for all of required training epochs - and we shuffle the quotes for each epoch.\n",
    "\n",
    "Finally, there is another reason to include the delimeter `<|endoftext|>`. The model will learn to produce a quote after it and also end the quote with it. Therefore we can prompt a fine-tuned model with `<|endoftext|>` to produce a new motivational quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZe0CBQlWb47"
   },
   "outputs": [],
   "source": [
    "class LinesTextDatasetWithEpochs(Dataset):  # based on TextDataset by Huggingface\n",
    "  def __init__(self, examples, tokenizer, block_size, num_epochs, example_del=\"<|endoftext|>\"):\n",
    "    super(LinesTextDatasetWithEpochs, self).__init__()\n",
    "    examples_input_ids = []\n",
    "    for ex in examples:\n",
    "      # we add the delimeter to each quote, tokenize it and convert the tokens to indices\n",
    "      examples_input_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(example_del + ex)))\n",
    "\n",
    "    # for each of the training epochs shuffle the quotes and combined them\n",
    "    combined_input_ids = []\n",
    "    for i in range(num_epochs):\n",
    "      tmp = examples_input_ids.copy()\n",
    "      random.shuffle(tmp)\n",
    "      combined_input_ids.extend(list(itertools.chain.from_iterable(tmp)))\n",
    "\n",
    "    # creating training samples by cutting the combined input into blocks of length block_size\n",
    "    self.data = []\n",
    "    for i in range(0, len(combined_input_ids) - block_size + 1, block_size):\n",
    "      self.data.append(tokenizer.build_inputs_with_special_tokens(combined_input_ids[i: i + block_size]))\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    return torch.tensor(self.data[i], dtype=torch.long)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3r6Hw9ouBr3"
   },
   "source": [
    "Below we implement the Trainer class that contains the main train loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7NdMQSrWdz3"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  def train(self, train_dataset, val_dataset, device, run_config):\n",
    "    self.model = self.model.to(device)\n",
    "    # create output folder if it doesn't yet exist\n",
    "    if not os.path.isdir(run_config.output_dir): \n",
    "      os.makedirs(run_config.output_dir)\n",
    "    \n",
    "    # train dataloader will serve us the training data in batches\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), \n",
    "                                  batch_size=run_config.batch_size, collate_fn=run_config.collate_fn)\n",
    "    \n",
    "    # optimizer and scheduler that modifies the learning rate during the training\n",
    "    optimizer = AdamW(self.model.parameters(), lr=run_config.learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=run_config.num_warmup_steps,\n",
    "                                                num_training_steps=len(train_dataloader)*run_config.num_epochs)\n",
    "    \n",
    "    print(\"Training started:\")\n",
    "    print(f\"\\tNum examples = {len(train_dataset)}\")\n",
    "    print(f\"\\tNum Epochs = {run_config.num_epochs}\")\n",
    "\n",
    "    global_step = 0  # to save after every save_steps if save_steps is >= 0\n",
    "\n",
    "    train_iterator = trange(0, int(run_config.num_epochs), desc=\"Epoch\")\n",
    "    for epoch in train_iterator:\n",
    "      epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "      self.model.train()\n",
    "      epoch_losses = []\n",
    "      for step, inputs in enumerate(epoch_iterator):\n",
    "        # move batch to GPU\n",
    "        if isinstance(inputs, dict):\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "        # forward pass - model also outputs a computed loss\n",
    "        outputs = self.model(**inputs)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # backward pass - backpropagation\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_iterator.set_description(f\"Training loss = {loss.item():.4f}\")\n",
    "\n",
    "        if run_config.save_steps > -1 and global_step > 0 and global_step % run_config.save_steps == 0:\n",
    "          output_dir = os.path.join(run_config.output_dir, f\"Step_{step}\")\n",
    "          self.model.save_pretrained(output_dir)\n",
    "          test_loss = self.evaluate(self.model, val_dataset, device, run_config)\n",
    "          print(f\"After step {step + 1}: val loss ={test_loss}\")\n",
    "\n",
    "        global_step += 1\n",
    "      \n",
    "      if run_config.save_each_epoch:\n",
    "        output_dir = os.path.join(run_config.output_dir, f\"Epoch_{epoch + 1}\")\n",
    "        model.save_pretrained(output_dir)\n",
    "        test_loss = self.evaluate(self.model, val_dataset, device, run_config)\n",
    "        print(f\"After epoch {epoch + 1}: val loss ={test_loss}\")\n",
    "\n",
    "\n",
    "  def evaluate(self, model, test_dataset, device, run_config):\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset),\n",
    "                                 batch_size=run_config.batch_size, collate_fn=run_config.collate_fn)\n",
    "    self.model.eval()\n",
    "    losses = []\n",
    "    for inputs in tqdm(test_dataloader, desc=\"Evaluating\", position=0, leave=True):\n",
    "      # move batch to GPU\n",
    "      if isinstance(inputs, dict):\n",
    "        for k, v in inputs.items():\n",
    "          inputs[k] = v.to(device)\n",
    "      else:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        loss = model(**inputs)[0]\n",
    "      losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Kjp4_KzVO2"
   },
   "source": [
    "*RunConfig* holds the parameter for training/testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTwJ1s27Wf1u"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunConfig:\n",
    "  learning_rate: float\n",
    "  batch_size: int\n",
    "  num_epochs: int\n",
    "  num_warmup_steps: int = 1\n",
    "  save_steps: int = -1\n",
    "  save_each_epoch: bool = True\n",
    "  output_dir: str = \"/content/\"\n",
    "  collate_fn: None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCkx8cHQznNV"
   },
   "source": [
    "## Training\n",
    "\n",
    "We have now implemented everything to start fine-tuning. We can save the fine-tuned models to our Colab instance (available under `/content/`) or we can connect our Google Drive to Colab and use it as external memory. If you want to do the latter, run the cell below and follow instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyJeCVJkWhcJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbJXEnFS0rQU"
   },
   "source": [
    "Google Drive is now accessable under `/content/drive/`.\n",
    "\n",
    "Let's prepare the datasets, tokenizer and training parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4d5g1X6WjXH"
   },
   "outputs": [],
   "source": [
    "block_size = 128  # length of input samples\n",
    "num_dataset_epochs = 8  # used to create the dataset, during training we'll only use 1 epoch\n",
    "\n",
    "# split the examples to train and validation sets\n",
    "train_examples, valid_examples = train_test_split(examples, test_size=0.2)\n",
    "\n",
    "# instantiate a GPT2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# datasets and the collate function to combine examples to input batches\n",
    "train_dataset = LinesTextDatasetWithEpochs(train_examples, tokenizer, block_size, num_dataset_epochs)\n",
    "val_dataset = LinesTextDatasetWithEpochs(valid_examples, tokenizer, block_size, 1)\n",
    "collate_call = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLV4CmPqWlpN"
   },
   "outputs": [],
   "source": [
    "run_config = RunConfig(\n",
    "    learning_rate = 3e-5,\n",
    "    batch_size = 32,  # start with 32 and decrease if you get CUDA out of memory exception\n",
    "    num_epochs = 1,  # the dataset already encodes the epochs\n",
    "    save_steps = len(examples) / (32 * 8),  # super ugly, but it just means we're saving after each epoch in dataset\n",
    "    save_each_epoch = True,\n",
    "    output_dir = \"/content/drive/My Drive/NLP-workshop/GPT2/\",\n",
    "    collate_fn = collate_call\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w659un9U6MFi"
   },
   "source": [
    "Let's instantiate the pretrained GPT-2 model. We are using the small version of GPT-2: 12 layers, 768 hidden dimension, 12 attention heads which combines for 117M parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoBPiitnWnhF"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxSCfeA9Wspz"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model)\n",
    "trainer.train(train_dataset, val_dataset, device, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8KhjhlTAgKM"
   },
   "source": [
    "If you happen to get a CUDA out of memory exception, do the following:\n",
    "- cause another exception so python doesn't hold any references to trainer or model, e.g. run the bottom cell causing ZeroDivisionError\n",
    "- run the cell below that empties GPU cache\n",
    "- decrease the batch_size in run_config and rerun that cell\n",
    "- reinstantiate the model and rerun training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuIJuYDvWuW5"
   },
   "outputs": [],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBkWNvfTWv4B"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "trainer = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoPifccwCRtQ"
   },
   "source": [
    "For the purposes of this workshop we already fine-tuned a GPT-2 model, let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mEOAQWOWxe_"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/gpt2-quotes\n",
    "!gdown -O /content/gpt2-quotes/config.json https://drive.google.com/uc?id=10h6le0yxZ8z-HJwOmWgSFcf2eXI5im-v\n",
    "!gdown -O /content/gpt2-quotes/pytorch_model.bin https://drive.google.com/uc?id=10kYtFp6tFRQClbLCi04xlGg-uXp3K3rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w22PxUkWzI0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"/content/gpt2-quotes/\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5NpZZ_6BxMl"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "We now have a fine-tuned GPT-2 model ready to produce motivational quotes. GPT-2 outputs a probability distribution over the next token conditioned on previous ones. There are a couple of ways we can go about generating text:\n",
    "- Greedy decoding\n",
    "- Beam search\n",
    "- Top-k/Top-p sampling\n",
    "\n",
    "You can read more [here](https://huggingface.co/blog/how-to-generate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNxd_mGVUe6o"
   },
   "source": [
    "#### Greedy decoding\n",
    "This is the simplest approach, at every step we just select the most probable next word, i.e. the word with highest outputed probability. One can immediately see that after some text the model will start repeating itself. This would therefore be a bad decoding scheme if we want to produce long continuous text, but since we're producing fairly short quotes it might achieve okay results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjRjJt_8W1Lc"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.generation_utils\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xK-k5zYW2pc"
   },
   "outputs": [],
   "source": [
    "def generate_text_greedy(prompt=\"\", max_length=64):\n",
    "  model.eval()\n",
    "  input_ids = tokenizer.encode(\"<|endoftext|>\" + prompt, return_tensors='pt').to(device)\n",
    "  generated_ids = model.generate(input_ids, max_length=max_length).cpu().tolist()\n",
    "\n",
    "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suVEaNXaW4Or"
   },
   "outputs": [],
   "source": [
    "generate_text_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2BKZDfwGRqh"
   },
   "source": [
    "So deep. Since greedy decoding is deterministic, this is the only quote produced by default prompt, but we can extend the prompt with some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mGfrYgkW6L4"
   },
   "outputs": [],
   "source": [
    "print(generate_text_greedy(\"I believe\"))\n",
    "print(generate_text_greedy(\"Data science\"))\n",
    "print(generate_text_greedy(\"Just\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdRBlBHsIJgp"
   },
   "source": [
    "#### Beam search\n",
    "\n",
    "Beam search is also a deterministic decoding, but offers an improvement over greedy decoding. A problem of greedy decoding is that we might miss the most likely sequence since we predict only most probably words. Beam search mitigates this by keeping a track of most probable *n* sequences at every step and ultimately selecting the most probable sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1w1p4WgXBNO"
   },
   "outputs": [],
   "source": [
    "def generate_text_beam(prompt=\"\", max_length=64, num_beams=8):\n",
    "  model.eval()\n",
    "  input_ids = tokenizer.encode(\"<|endoftext|>\" + prompt, return_tensors='pt').cuda()\n",
    "  generated_ids = model.generate(input_ids, max_length=max_length, num_beams=num_beams,\n",
    "                                 no_repeat_ngram_size=2).cpu().tolist()\n",
    "\n",
    "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TacICaPuXEH5"
   },
   "outputs": [],
   "source": [
    "generate_text_beam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5loQbjVaXGjn"
   },
   "outputs": [],
   "source": [
    "print(generate_text_beam(\"I believe\"))\n",
    "print(generate_text_beam(\"Data science\"))\n",
    "print(generate_text_beam(\"Just\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5rvB4XWJflA"
   },
   "source": [
    "#### Top-k/Top-p sampling\n",
    "\n",
    "We've looked at two deterministic decoding schemes, let's now focus on non-deterministic that is based on sampling the next word from a probability distribution. Outputed probability distribution is over the entire model vocabulary (order of tens of thousands), it has most of its mass on a subset of most probable words and a very long tail. The tokens in the tail part would produce incoherent gibberish therefore we must somehow limit ourselves to only sample from most probable words, that's where top-k and top-p sampling come into play:\n",
    "\n",
    "- [Top-k sampling](https://arxiv.org/abs/1805.04833) selects *k* most probable words and distributes their comulative probability over them. The problem is that we must choose a fixed sized parameter *k* which might lead to suboptimal results in some scenarios.\n",
    "- [Top-p sampling](https://arxiv.org/abs/1904.09751) addresses this by selecting top words whose cumulative probability just exceeds p. This comulative probability is then again distributed among these words.\n",
    "\n",
    "We'll use a combination of both in this notebook, but you're free to test different scenarios.\n",
    "\n",
    "There is another parameter that we haven't introduced: `temperature` which controls the outputed distribution from softmax function. Regular softmax has `temperature` = 1. If `temperature` -> 0, we give more probability mass to more probable words (we go towards greedy decoding). Higher values cause a more uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6EY1wrAXITB"
   },
   "outputs": [],
   "source": [
    "def generate_text_sampling(prompt=\"\", max_length=64, top_k=50, top_p=0.95, temp=1.0, num_return=1):\n",
    "  model.eval()\n",
    "  input_ids = tokenizer.encode(\"<|endoftext|>\" + prompt, return_tensors='pt').cuda()\n",
    "  generated_ids = model.generate(input_ids, do_sample=True, max_length=max_length, temperature=temp, \n",
    "                                 top_k=top_k, top_p=top_p, num_return_sequences=num_return).cpu().tolist()\n",
    "\n",
    "  generated_text = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chYFL3fUXJ5k"
   },
   "outputs": [],
   "source": [
    "generate_text_sampling(num_return=3, temp=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVQGxek7XL4q"
   },
   "outputs": [],
   "source": [
    "print(generate_text_sampling(\"I believe\", num_return=3, temp=0.7))\n",
    "print(generate_text_sampling(\"Data science\", num_return=3, temp=0.7))\n",
    "print(generate_text_sampling(\"Just\", num_return=3, temp=0.7))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMj6xaTUeHw10lNnzoxuSew",
   "collapsed_sections": [],
   "name": "quotes.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
