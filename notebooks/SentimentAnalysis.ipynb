{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSobi1Ca4i0W"
   },
   "source": [
    "# Sentiment analysis - fine-tuning BERT\n",
    "\n",
    "In this notebook we'll take a look at the process needed to fine-tine a pretrained [BERT](https://arxiv.org/abs/1810.04805) model to detect sentiment of a piece of text. Our goal will be to classify the polarity of IMDB movie reviews, we'll be working with a dataset from this [Kaggle source](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/notebooks). The techniques we'll discuss don't only apply for sentiment classification, but also for general text classification.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/andrejmiscic/NLP-workshop/raw/master/figures/classification.PNG\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKH-fJ4BqHmB"
   },
   "source": [
    "First things first, let's make sure we have a GPU instance in this Colab session:\n",
    "- `Edit -> Notebook settings -> Hardware accelerator` must be set to GPU\n",
    "- if needed, reinitiliaze the session by clicking `Connect` in top right corner\n",
    "\n",
    "After the session is initilized, we can check our assigned GPU with the following command (fingers crossed it's a Tesla P100!!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y-rUMHAnwBZ"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMI65N3GGnkr"
   },
   "source": [
    "Let's install some additional libraries: *transformers* for BERT implementation and *gdown* for loading from Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD7uSzrDj7v0"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/andrejmiscic/NLP-workshop/raw/master/utils/text_classification_utils.py\n",
    "!wget https://github.com/andrejmiscic/NLP-workshop/raw/master/utils/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "777JEt-8nyLi"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jos1ATjtnyur"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from text_classification_utils import TextClassificationDataset, collate_batch_to_tensors, seq_cls_evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trainer import Trainer, RunConfig\n",
    "from transformers import DistilBertConfig, DistilBertModel, DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaUnEp2fbQWp"
   },
   "source": [
    "## Data\n",
    "\n",
    "Let's take a look at our dataset of IMDB reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ten6sF7vn55k"
   },
   "outputs": [],
   "source": [
    "path_to_train_csv = \"https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/IMDB-reviews/imdb_train.csv\"\n",
    "\n",
    "df = pd.read_csv(path_to_train_csv)\n",
    "class_list = sorted(df[\"label\"].unique().tolist())\n",
    "label2id = {label: i for i, label in enumerate(class_list)}\n",
    "id2label = {i: label for i, label in enumerate(class_list)}\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(df.head())\n",
    "print(f\"Classes: {class_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PigdQ0EHc4aL"
   },
   "source": [
    "Notice that our reviews fall into two polarity classes: *positive* and *negative*. This is therefore a binary sequence classification task. Below we prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erGAUrj0-apm"
   },
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"label\"].map(label2id)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"])\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "max_len = 512\n",
    "train_dataset = TextClassificationDataset(train_df[\"text\"].tolist(), train_df[\"label\"].tolist(), tokenizer, max_len)\n",
    "val_dataset = TextClassificationDataset(val_df[\"text\"].tolist(), val_df[\"label\"].tolist(), tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqGzX_qeeBC1"
   },
   "source": [
    "## Model\n",
    "\n",
    "Same as for Named Entity Recognition we are working with DistilBERT, a smaller model than base BERT, that is though by knowledge distillation and retains most of the performance. As mentioned during the lectures, BERT has a special token (*CLS*) whose representation we use as inputs to a classifier. During pretraining this is trained on the task of next sentence prediction therefore out of the box it is not useful as sequence representation. That's where finetuning comes in - we train a classifier together with pretrained BERT model to achieve good performance.\n",
    "\n",
    "An architecture for sequence classification is already impemented in *transformers* library: [*DistilBertForSequenceClassification*](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification), but for demonstrational purposes we reimplement a DistilBERT with a classification head below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVaOgbTloLnp"
   },
   "outputs": [],
   "source": [
    "class DistilBertTextClassificationModel(nn.Module):\n",
    "    def __init__(self, bert_config, num_classes, dropout_prob=0.1):\n",
    "        super(DistilBertTextClassificationModel, self).__init__()\n",
    "\n",
    "        self.bert = DistilBertModel(bert_config)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classification_layer = nn.Linear(in_features=bert_config.hidden_size, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask)[0]\n",
    "        cls = outputs[:, 0, :]  # [CLS] is the first token of the sequence\n",
    "        cls = self.dropout(cls)  # to mitigate overfitting\n",
    "        logits = self.classification_layer(cls)  # classify\n",
    "\n",
    "        if labels is None:\n",
    "          return logits\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return (loss, logits)\n",
    "\n",
    "    def load(self, path_to_dir):\n",
    "        self.bert = DistilBertModel.from_pretrained(path_to_dir)\n",
    "        model_path = os.path.join(path_to_dir, \"model.tar\")\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            self.dropout.load_state_dict(checkpoint[\"dropout\"])\n",
    "            self.classification_layer.load_state_dict(checkpoint[\"cls\"])\n",
    "        else:\n",
    "            print(\"No model.tar in provided directory, only loading bert model.\")\n",
    "\n",
    "    def save_pretrained(self, path_to_dir):\n",
    "        self.bert.save_pretrained(path_to_dir)\n",
    "        torch.save(\n",
    "            {\"dropout\": self.dropout.state_dict(), \"cls\": self.classification_layer.state_dict()},\n",
    "            os.path.join(path_to_dir, \"model.tar\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLDp9pJvk_IP"
   },
   "source": [
    "## Training\n",
    "\n",
    "We have now implemented everything to start fine-tuning. We can save the fine-tuned models to our Colab instance (available under `/content/`) or we can connect our Google Drive to Colab and use it as external memory. If you want to do the latter, run the cell below and follow instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zPBX4qloQYz"
   },
   "outputs": [],
   "source": [
    "# optional if you want to save your models to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJTtkmhToRog"
   },
   "outputs": [],
   "source": [
    "run_config = RunConfig(\n",
    "    learning_rate = 3e-5,\n",
    "    batch_size = 32,  # start with 32 and decrease if you get CUDA out of memory exception\n",
    "    num_epochs = 3,\n",
    "    output_dir = \"/content/drive/MyDrive/NLP-workshop/BERT-sentiment/\",\n",
    "    collate_fn = collate_batch_to_tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfMcapBclKuR"
   },
   "source": [
    "Instatiate the model and start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQTMCKohoVsO"
   },
   "outputs": [],
   "source": [
    "model = DistilBertTextClassificationModel(\n",
    "    DistilBertConfig.from_pretrained(\"distilbert-base-uncased\"), \n",
    "    num_classes=len(class_list)\n",
    ")\n",
    "model.load(\"distilbert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbYnFUoroXMD"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model)\n",
    "trainer.train(train_dataset, val_dataset, device, run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqHqkCUBlNIB"
   },
   "source": [
    "If you happen to get a CUDA out of memory exception, do the following:\n",
    "- cause another exception so python doesn't hold any references to trainer or model, e.g. run the bottom cell causing ZeroDivisionError\n",
    "- run the cell below that empties GPU cache\n",
    "- decrease the batch_size in run_config and rerun that cell\n",
    "- reinstantiate the model and rerun training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8-VI7lSoY2h"
   },
   "outputs": [],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xib_4trnobL8"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "trainer = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsmORIJVlZLl"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "With this procedure we've now fine-tuned a model to predict the polarity of a review. For the purposes of this workshop we've pretrained a model so we can analyze it. Load it by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30QoRzNOocjC"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/bert-imdb\n",
    "!gdown -O /content/bert-imdb/config.json https://drive.google.com/uc?id=1-5Z1EvvyYdXr73fQf7_nTyeltewWt5WV\n",
    "!gdown -O /content/bert-imdb/model.tar https://drive.google.com/uc?id=1-S8Ii5SeazeqOWtI0Wx9JiSap5-mV4GQ\n",
    "!gdown -O /content/bert-imdb/pytorch_model.bin https://drive.google.com/uc?id=1-R-lyZL53rY5wdfW2IWKUPFNDznc61QP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdl_VTI9l-M0"
   },
   "source": [
    "Let's instantiate all the objects we need for evaluation: model, dataset, tokenizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL3TVSuOAhsF"
   },
   "outputs": [],
   "source": [
    "path_to_test_csv = \"https://raw.githubusercontent.com/andrejmiscic/NLP-workshop/master/Data/IMDB-reviews/imdb_test.csv\"\n",
    "\n",
    "df = pd.read_csv(path_to_test_csv)\n",
    "class_list = sorted(df[\"label\"].unique().tolist())\n",
    "label2id = {label: i for i, label in enumerate(class_list)}\n",
    "id2label = {i: label for i, label in enumerate(class_list)}\n",
    "df[\"label\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n",
    "max_len = 512\n",
    "test_dataset = TextClassificationDataset(df[\"text\"].tolist(), df[\"label\"].tolist(), tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dE-gcCzCodv6"
   },
   "outputs": [],
   "source": [
    "# only run if you want to use the model we've already fine-tuned for you\n",
    "model = DistilBertTextClassificationModel(\n",
    "    DistilBertConfig.from_pretrained(\"distilbert-base-uncased\"), \n",
    "    num_classes=len(class_list)\n",
    ")\n",
    "model.load(\"/content/bert-imdb/\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNGJ0UwHmKyC"
   },
   "source": [
    "Evaluating our fine-tuned model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqSWgtnFogiz"
   },
   "outputs": [],
   "source": [
    "log_loss, accuracy = seq_cls_evaluate(model, test_dataset, device, batch_size=64)\n",
    "print(f\"\\nTest log loss = {log_loss:.4f}\\nTest accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7wYCI1Km_qi"
   },
   "source": [
    "Nice, we achieve a relatively good accuracy of 0.93. We can now experiment with the model and write some custom reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtnluYjeoh5i"
   },
   "outputs": [],
   "source": [
    "def predict_review_sentiment(review: str):\n",
    "  enc = tokenizer(review)\n",
    "  inputs = {\"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device),\n",
    "            \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)}\n",
    "  with torch.no_grad():\n",
    "    prediction = np.argmax(nn.functional.softmax(model(**inputs), dim=1).cpu().numpy())\n",
    "  print(review)\n",
    "  print(f\"Sentiment: {id2label.get(prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPV9BXL4lDoO"
   },
   "outputs": [],
   "source": [
    "predict_review_sentiment(\"I think this movie is good.\")\n",
    "predict_review_sentiment(\"I don't think this movie is good.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_sentiment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
